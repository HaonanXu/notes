log compaction: kafka concept
deleting old segemnts of log works well for temporal systems
but what if only the last state matters => log compaction: support retaining messges by key instead of purely by time.
usage:
maintain a replica of the data stream by key,e.g, indexing, cache
Event sourcing: each occurence is maintained and various devrived values are computed off of this
journally arbitrary processing

Kafka acts as source of truth, so it is fine if upstream source is not replayable

message with a key and null payload will be treated as a delete from the log. any prior message with that key will be removed. NOTE: delete markers will themselves be cleaned out of the log after a period of time

compaction is done in the background by periodically recopying log segments, can be throttled
deletion retention SLA to handle race condition

log compaction is a physical optimization, similar to caching,i.e., we should assume log tail doesnt exist when coding at the beginning. But we need to factor log compaction in when we do performance and handling deletions
