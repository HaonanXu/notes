simple rewind: restaring the job at an older point in time in the stream, and running through all messages since that time

parallel rewind:
run two jobs in parallel: one job continues to handle live updates with low latency while the other is started at an older point in the stream => eventually the reprocessing job catches up with the real-time job

to jump back, you can
1. stop the job and change the last check points
2. start a new job with a different job.name or job.id => but it will start at the oldest offset available

In most cases, a job that jumps back in time should start with an empty state. can reset state by deleting the topic, or by changing the name of the topic in job config

common to kafka brokers to keep one or two weeks of message history acceissible, even for high volume topics => can handle TBs of history

activity events: time-based retention, events older than the retention period are deleted or archived in HDFS

database changes: last change matters => changelog streams used by samza's persistent state => but need to jump to the beginning of time with log compaction

To implement parallel rewind
1. need different job.name or job.id
2. need different changelog to maintain persistant state
3. each job to have separarte output stream, or if you use same output, newer data can NOT be overriden by older data
4. new output test case ideal for A/B testing
5. eventually need to shut down the old job, delete the checkpoint and changelog streams belonging to the old version

------
Problem: 
If a job is accumulative, i.e., later results depend on previous result, how do we handle reprocessing?
Scenario:
User makes corrections to tax return filed in the past, all the following years credit/balance needs to be recalculated.

What is the condition:
fault tolerance,i.e., updating state and persist state locations. message most likely needs to be idempotent

What is known?
1.
persistant state for samza: k-v store
stored on disk, on the same machine as the stream task. 
replicated for fault tolerance. No problem check pointing large amounts of state
all writes to the database partition are replicated to a durable kafka. So when a machine fails, we can restart the tasks on another machine, and consume the changelog to restore the contents of the database partition (50 MB/sec)
2.
log compaction:
A kafka optimization we can exploit but not rely on

From unknown
When we reach the correction to previous facts, we need to recompute later results,i.e., we need to know later facts to begin with, so they must be available as local state

Swap unknown and known
After we update this period's result, the updated result will propergate to the output topic as well, forcing the follow up consumer to update their old result, i.e., this "backtracking" of facts is a cascading and is a common pattern

Therefore, every time we see an entry, we need to recompute ALL periods starting from the current one, and output all updated results to the result. 

How is local state organized?
Depends on what is the rate of this backtracking change. Note that similar to map-reduce, such stateful stream processing logic requires deep coupling with business rules to be efficient

Potential optimization
batch processing
may be able to compute only deltas
similar to the idea of log compaction, set up rules so only last change matters
