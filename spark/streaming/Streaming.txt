Operations:

repartition: changes the level of parallelism
reduce: function should be associative 
countByValue:
reduceByKey:
join: note that they must be within the same stream batches
cogroup:
When called on DStream of(K,V) and (K,W) pairs, return a new DStream of(K, Seq[V], Seq[W]) tuples

updateStateByKey: state for each key is updated by applying the givne function on the previous state of the key => need to turn on updateStateByKey

transform: rdd to rdd operations, the function provided to tranform is evaluated every batch interval

print(): first ten elements of every batch of data

saveAsTextFiles|ObjectFiles|HadoopFiles

foreachRDD: push the data in each RDD to a external system => problems and patterns
-------
can convert to DataFrame when inside foreachRDD, and use dataframe ops

---
metadata checkpointing: savng straming computation def into HDFS: to recover from failure of the node running the driver

data checkpointing: save generated rdds. Necessary in stateful transformation the combine data acoross multiple batches.

enable it by setting a dir of HDFS to which the checkpoint info will be saved:  streamingContext.checkpoint(checkpointDirectory): this
means
1. When the program is being started for the first time, it will create a new StreamingContext, set up all the streams and then call start()
2.When the program is being restarted after failure, it will re-create a StreamingContext from the checkpoint data in the checkpoint
directory 

=> use StreamingContext.getOrCreate

Deployment need to ensure that driver process gets restarted automatically on failure

Interval of checkpointsin needs to be set carefully, start iwth checpoit interval of 5-10 sliding intervals.
updateStateByKey: seems API is changed already. a new State object is introduced => trackStateByKey

transform Ops to do stream joins => base rdd joins??

sliding windows: window length and sliding length

Checkpointing
------
metadata checkpointing: recover from driver failure. But need to use StreamingContext.getOrCreate(checkpointDir,  functionToCreateContext _)
pattern so stream will be recreated from checkpoint data

Data checkpointing: required stateful ops

------
traditional model; continous operators model, process the data one record at a time
potential problems:
failure recovery
load balancing
unification of streaming, batch and interactive workload

in table-stream join, use DStream.transform

streaming rdd to DF
