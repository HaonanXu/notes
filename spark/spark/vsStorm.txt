Storm:
focused on event processing, eals with each event individually. Task-Parrallel
sub-sec latency
At least once delivery, need to track record as they move
written in Closurehttp://10.50.22.43:10080/spmlab/vagrant-kafka
Need trident library to simulate spark batch processing behavior
not direclty on HDFS, useshttp://10.50.22.43:10080/spmlab/vagrant-kafka Zookeeper
use Netty as message channel
relies on remote database for durable storage
one thread per task

Spark: Data-parallelhttp://10.50.22.43:10080/spmlab/vagrant-kafka
Batches input data, then perform batch processing. a DStream is represented as a sequence of RDDs. Batch interval set based on the latency requirements of application
Also supports Akka http://10.50.22.43:10080/spmlab/vagrant-kafkaactors as basic sources, Kafka as advanced sources
after that start(), awaitTermination()/stop()
Once a context has started, no new streaming computations can be set up/added
once a context has stopped, can not be restarted
Only one streaming context cna be active in a JVM at the same time
receiver takes core, need to leave cores for processing as well
Reliable receiver: correctly acknowlesgs a reliable source that the data has been received and stored in Spoark with replication



State computation: UpdateStateByKey: requires checkpointing
transfrom operation: RDD to RDD
Window function: reduceByKeyAndWindow
Joins:
Output: textFile, HaddopFiles, foreachRDD and its patterns, output operations are executed one-at-a-time

metadata and data checkpointing: 5-10 times of sliding interval of a DStream
http://10.50.22.43:10080/spmlab/vagrant-kafka
falut toleration: if base RRD is FT, some is the transformed.

seconds of latency
each batch is proccessed exactly once
written in scala
has YARN support which is not available to Storm. Also has standalone(spark) mode
Spark streaming failure scenario: write ahead log + fault tolerate requires a data source that can replay data (e.g., kafka)
Spark uses combination of Netty and Akka
Standalone resource manager handles master node failure with staby-mastes and ZooKepper

Samza:
stream join better, Spark stream's join has can join only within 2 intervals
unlike storm/spark's topology idea, Samza: each job is just a message at-a-time procesor
output of a procesing task alwasy needs to go back to a message broker
always process,s mesags in the order they appear in the partition, no guarantee of ordering across different input streams of partitionsno such guarantee in Storm
so it is better for keyed data processing
each task includes an embedded k-v store on the same machine
single-threaded processes for container

All 3 support Mesos as resource manager
