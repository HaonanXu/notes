To pass functions, so that RDD can use it on a cluster
1. anonymous function
2. state methods in a global singlen object

if pass a referenct ot a method in a class instance of object, this means whole object will be serialized and sent.
One way to avoid it is to copy the this into a local closure

Use Accumulator to update varaible when execution is split up across worker nodes
=> closures should not be used to mutate some global state

similarly rdd.foreach(println) will use the stdout closure avaialbe at executer, so driver is not able to see this!
will have to use collect(), if the whole dataset can fit into memeory

because of shuffle operations, such as reduce or sort, need to override equals and hashCode

creating braodcast variable is only useful when task across multiple stages need the same data or when caching the data in deserialized form
is importatnt,e.g., joining a large and small RDD, use medium RDD's keys to try to filter large RDDs where there is no matching key

accumulators: varaibles and associate operation

Inside action, eash tak's update ot the accumulator wil only be applied once, i.e., restarted tasks will not update the lvaue.
In transfomrations, each tasks' update may be paplied more than once in case of re-execution

--------
if lazy val is uninited, lazy val will be recalculated after de-serialization

@transient lazy val can be used to hanlde object that are serializable but do not want ot recrate too often,i.e., you just ask class to
carry enough info to reconstruct them

----
Working with unserializable class, but need that in RDD
1. serializable wrapper
2. thunk
3. use Externalizable

----
The only place you can access rdd is the driver

example: Kafka Spark producer optimization and patterns
