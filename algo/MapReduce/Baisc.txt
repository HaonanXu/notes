
r reducers write to r output files in HDFS, and content is sorted by reducer
key in each file

backup task/speculative execution: map stage is determined by the slowest map
task(Shuffle and sort can not start until all map jobs are done) So exe a copy of that and take the first one that completes

How to handle reducer straggler?

each intermediate value arrives at reducer in sorted key order. but there is no
ordering between reducers

HDFS stores relative few number of large (multi-gig) files. Since name node
has to hold it in memory

jobs are batch oriented: long streaming R/W. High bandwidth is more important than low-latency

HDFS assumes only authorized user can access it,i.e., no built-in tight
security

------
What user can do with MR jobs

1. complex data structure as k/v
2. custome init/finish code in mapper/reducer
3. mapper and reducer are stateful
4. sort order of intemediate keys
5. partition of the key space => which set a reducer will run on

local aggregation:
1. combiner. Do some reducer worker on mapper's node
2. in mapper combining. explicitly do the combiner work in map function. May
need to flush our k/v pairs from time to time.
Note often it is not optimal to search for optimal buffer size, because
mutliple tasks may run on the same node, and these tasks dont know each other
May help reducer straggler problem as well, because the chokepoint's work is
now distributed among mappers

combiner is viewed as optimization => may or may not be executed. Therefore,
the input AND output type of the combiner must be same as the output time of
mapper

Example: calculate mean with combiner
