One way to prove is the show we can improve the step by changing the solution in the direction of our step, i.e., similar to the replacement
proof in greedy. A concrete example will be gradient descent
=> somewhat similar to relaxation, except that relaxation has a context focus,i.e., moving to a new state enables new information available,

total # of steps will be bounded by the dimension of data.=> even a while loop would do

"as good as any other solution"

---
For existence proof, we can just show that the extreme value works, and all general cases can be reduced to extreme cases with additional
(non-intefering) parameters, basically we use extreme case as a subproblem to introduce additioal conditions, since too hard to see patterns in general problem

One common problem is inequality constraints, such constraint can possibly satisfied <=> optimal values satify it!

-----

one pattern of generation vs filtering:  max(sums)  = Max possible value - MIN(sum of filtered outs)

----
extreme value problem can be reduced to search problems: search all possible states. this helps decompose the problem -> we just need to
focus on the new unkown: how can we find all possible states, what can we infer from the unknown/condition?
