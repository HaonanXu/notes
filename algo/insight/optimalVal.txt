One common problem is inequality constraints, such constraint can possibly satisfied <=> optimal values satify it!
When we have many optimal solutions with the same value, we can focus on the "optimal on a second dimension" of these optimal solutions.
This way we can greatly limit the number of alternative solutions we have to consider at each step.

one pattern of generation vs filtering:  max(sums)  = Max possible value - MIN(sum of filtered outs)

extreme value problem can be reduced to search problems: search all possible states. this helps decompose the problem -> we just need to
focus on the new unkown: how can we find all possible states, what can we infer from the unknown/condition?i.e., given the target, we can infer some properties and thus introduce a searchable space, going through that and update optimal and valid solution as we go.

since global optimal => local optimal, we can focus on global optimal as the starting point, and play with its properties

because optimal value is often only a single number, how do you decompose the unkown? One common techinque is to break down an assumed solution into parts/steps/and consider each part's contribution to the overall solution. Ideally, each part/case's contribution should be ISOLATED against each other at least in the optimal case, and then prove that you can reach an optimal contribution of each case at the same time => this suggests a greedy approach! 
