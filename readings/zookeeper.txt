things can use ZK for:config info, leader election, membership info, resource locking

designed for high throuput: 10k+ transactions per sec, read dominated

looks similar to a file system API

write: linearizable, atomic broadcast
read: handle locally

individual's async client are guaranteed FIFO order

if you pass a sequenctial flag, a monotonically increasing # will be included in file name, to guarantee ordering and uniqueness

-----
Patterns:

Configuration:
put info inside zcode, pass the path to a pass with watch, if the config is updated, watch will notify app config is stale

Group memebership:
Create a parent node, put memebers as ephemeral child nodes, if the process dies, then the node will automatically deleted. Enumerate the
group to see who are alive. Refresh the watch everytime you got a notification

Lock: tricky implementaiton!

Leader election: similar to locking
1. each member creates a sequential, ephemeral child node
2. lowest sequential # is the leader
3. each one watches the next lowest sequence # to its own

Updates are logged and forced to disk before applied in memory

fuzzy snapshot and why they work?

---------
From API perspective, ZK like chubby without open, close, and lock

why wait-free is not enough for coordniation

FIFO client ordering means client can do async + linearizable wirtes

server processes read locally, do not use zab to process them

ZK uses watch to enable client cache, Chubby manages client cache directly => blocks all updates to invalidate caches on all clients, and use lease to bound the effect of slow/faulty clients

watches are unregisterd once notified or session is closed

for an app server that is starting, needs to know which is leadr => store it in a known, pre-agreed znode

delete(path, version)
set(path, data, version)
sync(): waiting until all updates since start propagates to the server

Why not use handle to access znodes?

why zk can process read at each replica?

2 requirements after a new leader is just elected:
1. no partial read during config change
2. no partial read if leader dies in the middle
why chubby's lock approach is not sufficient?
how to solve this problem with ZK? and why would that work?
=>ordering guarantee: if client is watching for a change, it will be notified before being able to read any of new updates

problem with local-handled read request, and how to solve them?

liveness guarantee: majority
durability: change perists any # of failures as long as quorums of servers able to recover

Rendezvous: share final system config, e.g., dynamic master

Simple locks
Simple locks without herd effects
Read/write locks
Double barriers

Compare YMB ZK usage with kafka's

------
each znode stores max 1MB by default, force writes to disk before taking snapshots

why request processor has to calculate future state?

zab guarantees that broadcasted changes are in the order they are sent

need to use idempotent transaction

what is the consequence of fuzzy snapshots

server process writes in order => no concurrent write/read ops
server handles notification locally
read result has zxid, last transaction seen by the server

sync is ordered by leader after all pending changes to logical replica
client calles sync followed by read => why this would work?

if a client has a higher zxid, the server does not estabilihs connection until it catches up

heatbeat every s/3, switch to new server if client doesn't hear from current one in s *(2/3)
