Every node that provides services to Consul runs a Consul agent. Running an agent is not required for discovering other services or getting/setting key/value data. The agent is responsible for health checking the services on the node as well as the node itself.

Instead of only having server nodes, Consul clients run on every node in the cluster. These clients are part of a gossip pool which serves several functions, including distributed health checking. The gossip protocol implements an efficient failure detector that can scale to clusters of any size without concentrating the work on any select group of servers.

Consul loads all configuration files in the configuration directory, so a common convention on Unix systems is to name the directory something like /etc/consul.d (the .d suffix implies "this directory contains a set of configuration files

if building a new server with the same IP isn't an option, you need to remove the failed server. Usually, you can issue a consul force-leave command to remove the failed server if it's still a member of the cluster.

As a sanity check, the consul info command is a useful tool. It can be used to verify raft.num_peers is now 2, and you can view the latest log index under raft.last_log_index. When running consul info on the followers, you should see raft.last_log_index converge to the same value once the leader begins replication. That value represents the last log entry that has been stored on disk.

Each node in a cluster must have a unique name. By default, Consul uses the hostname of the machine

You can easily do this by containerizing the Consul agent software and launching it via an EC2 UserData script that is invoked when the ECS instance is launched. The agent needs to communicate with a Consul server that stores the service directory.

When running the Consul container on the default bridge network, an additional configuration parameter is required to enable host-to-host gossip between Consul agents. The parameter in question is the advertise parameter. It is intended for scenarios where the routable IP to the agent is not discoverable and needs to be manually configured. In the case of the container, we need the IP of the ECS cluster host instead of the container IP on the hostsâ€™ bridge network.

----------
A service definition is the most common way to register services

You'll notice in the output that it "synced" the web service. This means that the agent loaded the service definition from the configuration file, and has successfully registered it in the service catalog.

Service definitions can be updated by changing configuration files and sending a SIGHUP to the agent. This lets you update services without any downtime or unavailability to service queries.

Alternatively, the HTTP API can be used to add, remove, and modify services dynamically.

We don't cover Consul's multi-datacenter capability here, but as long as --net=host is used, there should be no special considerations for Docker.

The cluster address is the address at which other Consul agents may contact a given agent. The client address is the address where other processes on the host contact Consul in order to make HTTP or DNS requests. You will typically need to tell Consul what its cluster address is when starting so that it binds to the correct interface and advertises a workable interface to the rest of the Consul agents.

Note that the agent defaults to binding its client interfaces to 127.0.0.1, which is the host's loopback interface. This would be a good configuration to use if other containers on the host also use --net=host, and it also exposes the agent to processes running directly on the host outside a container, such as HashiCorp's Nomad.

If you want to expose the Consul interfaces to other containers via a different network, such as the bridge network, use the -client option for Consul

check to see how to expose Consul's DNS Server on Port 53!

Registrator monitors the Docker daemon for container stop and start events, and handles service registration with Consul using the container names and exposed ports as the service information.

Consul has the ability to execute health checks inside containers. If the Docker daemon is exposed to the Consul agent and the DOCKER_HOST environment variable is set, then checks can be configured with the Docker container ID to execute in. See the health checks guide for more details.

If a container listens on multiple ports, it has multiple services.
