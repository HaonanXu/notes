Every node that provides services to Consul runs a Consul agent. Running an agent is not required for discovering other services or getting/setting key/value data. The agent is responsible for health checking the services on the node as well as the node itself.

Instead of only having server nodes, Consul clients run on every node in the cluster. These clients are part of a gossip pool which serves several functions, including distributed health checking. The gossip protocol implements an efficient failure detector that can scale to clusters of any size without concentrating the work on any select group of servers.

Consul loads all configuration files in the configuration directory, so a common convention on Unix systems is to name the directory something like /etc/consul.d (the .d suffix implies "this directory contains a set of configuration files

if building a new server with the same IP isn't an option, you need to remove the failed server. Usually, you can issue a consul force-leave command to remove the failed server if it's still a member of the cluster.

As a sanity check, the consul info command is a useful tool. It can be used to verify raft.num_peers is now 2, and you can view the latest log index under raft.last_log_index. When running consul info on the followers, you should see raft.last_log_index converge to the same value once the leader begins replication. That value represents the last log entry that has been stored on disk.

Each node in a cluster must have a unique name. By default, Consul uses the hostname of the machine

You can easily do this by containerizing the Consul agent software and launching it via an EC2 UserData script that is invoked when the ECS instance is launched. The agent needs to communicate with a Consul server that stores the service directory.

When running the Consul container on the default bridge network, an additional configuration parameter is required to enable host-to-host gossip between Consul agents. The parameter in question is the advertise parameter. It is intended for scenarios where the routable IP to the agent is not discoverable and needs to be manually configured. In the case of the container, we need the IP of the ECS cluster host instead of the container IP on the hostsâ€™ bridge network.

----------
We don't cover Consul's multi-datacenter capability here, but as long as --net=host is used, there should be no special considerations for Docker.

The cluster address is the address at which other Consul agents may contact a given agent. The client address is the address where other processes on the host contact Consul in order to make HTTP or DNS requests. You will typically need to tell Consul what its cluster address is when starting so that it binds to the correct interface and advertises a workable interface to the rest of the Consul agents.

Note that the agent defaults to binding its client interfaces to 127.0.0.1, which is the host's loopback interface. This would be a good configuration to use if other containers on the host also use --net=host, and it also exposes the agent to processes running directly on the host outside a container, such as HashiCorp's Nomad.

If you want to expose the Consul interfaces to other containers via a different network, such as the bridge network, use the -client option for Consul

check to see how to expose Consul's DNS Server on Port 53!


--------
Health checks and service registrations are independent of each other in Consul. They work closely together, but it is not strictly necessary that you deregister a service if the node(s) providing the service go offline. Queries via the DNS or HTTP health API's will not return the unhealthy node when querying for the service.
