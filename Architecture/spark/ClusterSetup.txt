1. etc/hadoop/hadoop-env.sh
export JAVA_HOME={java installation folder}
export HADOOP_PREFIX={absolute path of your hadoop folder}

2. etc/hadoop/core-site.xml

<property>
    <name>fs.defaultFS</name>
    <value>hdfs://{host name of name manager node}/</value>
 </property>

default port is 8020, or whatever shows
then in hdfs-site.xml 
 <property>
        <name>dfs.datanode.data.dir</name>
        <value>file://{local file system path1}, file://{local file system path2},file://{local file system path3},....</value>
 </property>

<property>
<name>dfs.name.dir</name>
 <value>/home/george/hadooptmp/tmp1,/home/george/hadooptmp/tmp2</value>
</property>


3. etc/hadoop/yarn-site.xml

<property>
  <name>yarn.resourcemanager.hostname</name>
  <value>{hostname of resource manager node}</value>
</property>

4. etc/hadoop/slaves
list ALL nodes hostname, including data node, name node, and resource node, 1 per line

6. /etc/hosts
we MAY need to add host name rules for ALL nodes in the cluster

6.5 
bin/hdfs namenode -format
bin/hdfs datanode -format

7. start nameManager on name node
sbin/hadoop-daemon.sh --script hdfs start namenode

8. for each data node
sbin/hadoop-daemon.sh --script hdfs start datanode

7-8 can use . sbin/start-dfs.sh

9. start resourceManager on resource node
sbin/yarn-daemon.sh start resourcemanager

10. for each node, start nodeManager
sbin/yarn-daemon.sh start nodemanager

we can probably skip 9-10 if we use only hdfs


------------
Spark configs

1. set these environment variable in EACH note's conf/spark-env.sh 

SPARK_EXECUTOR_MEMORY= {no more than 75% of actual ram}
SPARK_DRIVER_MEMORY=  
SPARK_WORKER_MEMORY= 
SPARK_LOCAL_DIRS={comma delimited local paths}


2.  create/modify conf/slaves, then use start-all.sh

or 
start spark master
./sbin/start-master.sh
 
start spark workers
./bin/spark-class org.apache.spark.deploy.worker.Worker spark://{master's hostname}:7077


----------
to invoke remote hdfs command
bin/hdfs dfs -fs hdfs://{master host}:8020 -ls /

make the following web ui ports public
hdfs Name node:50070
spark master node:8080
each spark woker node:8081
