1. etc/hadoop/hadoop-env.sh
export JAVA_HOME={java installation folder}
export HADOOP_PREFIX={absolute path of your hadoop folder}

2. etc/hadoop/core-site.xml

<property>
    <name>fs.defaultFS</name>
    <value>hdfs://{host name of name manager node}/</value>
 </property>

default port is 8020, or whatever shows

2.5 add mounting points to data nodes
for each data node,

spark recommends 4+ mounting points, i.e,

sudo chmod 777 -R /mnt/disk1
mkdir /mnt/disk1/dn

sudo chmod 777 -R /mnt/disk2
mkdir /mnt/disk2/dn

sudo chmod 777 -R /mnt/disk3
mkdir /mnt/disk3/dn

sudo chmod 777 -R /mnt/disk4
mkdir /mnt/disk4/dn

and then make sure hadoop user has permission to these /dn folders

then in hdfs-site.xml 
 <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///mnt/disk1/dn,file:///mnt/disk2/dn,file:///mnt/disk3/dn,file:///mnt/disk4/dn</value>
 </property>


3. etc/hadoop/yarn-site.xml

<property>
  <name>yarn.resourcemanager.hostname</name>
  <value>{hostname of resource manager node}</value>
</property>

4. etc/hadoop/slaves
list ALL nodes hostname, including data node, name node, and resource node, 1 per line

5. setup SSH
On name node and resource node

ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
ssh-copy-id {user}@{data node hostname}

Do this for each data node

repeat 1-5 for all nodes in the cluster

6. /etc/hosts
we MAY need to add host name rules for ALL nodes in the cluster

6.5 
bin/hdfs namenode -format
bin/hdfs datanode -format

7. start nameManager on name node
sbin/hadoop-daemon.sh --script hdfs start namenode

8. for each data node
sbin/hadoop-daemon.sh --script hdfs start datanode

7-8 can use . sbin/start-dfs.sh

9. start resourceManager on resource node
sbin/yarn-daemon.sh start resourcemanager

10. for each node, start nodeManager
sbin/yarn-daemon.sh start nodemanager

we can probably skip 9-10 if we use only hdfs


----
make the following web ui ports public

hdfs Name node:50070
spark master node:8080
each spark woker node:8081

start spark master
./sbin/start-master.sh

or, create/modify conf/slaves, then use start-all.sh
 
start spark workers
./bin/spark-class org.apache.spark.deploy.worker.Worker spark://{master's hostname}:7077

to invoke remote hdfs command
bin/hdfs dfs -fs hdfs://{master host}:8020 -ls /
