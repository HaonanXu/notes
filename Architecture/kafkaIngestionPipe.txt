Known:
Client calling will mostly in node.js
Client will read from one of the MySQL db: marketplace, OAuth, and potentially payment gatement/wallet?

Condition:
Writes to Kafka, in Avro format

Unknown:
design of SDK used by the client

From known:
SDK will mostly in node.js
since node works mostly with JSON, it is SDK's job to translate JSON to Avro (explicit or implicitly)
In Any avro solutions, a schema registry is something we will need eventually, might try to solve a more general problem by droping
conditions, i.e., kicking off the whole process of integrating confluence.io into our ingestion pipeline

A new sub-problem: how to integrate confluent.io?
schema registry can be added incrementally, i.e., might as well do it 

Insights:
1.the end goal for the SDK project is the producer to kafka part is completely through confluent.io
2.our sdk will be in node.js 

New sub problems from 1):
1. schema registery: how does it work, espeically fault tolerance and schema versions. Note that Avro schema versions require specific
upgrade steps to ensure forward/backward compatibility

From 2) new sub problems:
1.we know our SDK will be an NPM package, do we have a private npm registry?
2.How to test it? sub-problems by dropping conditions:
  1. how to do unit/integration testing in node in general
  2. how to do integration testing in node in conjuction with kafka
  3. how to do testing in node and confluence.io

=> We have to dive into details now to gain more insights

Even at this stage we can do interface design now
Solve related problems:
1. interface of vanilla node.js kafka producer?
2. interface of vanillla node REST request sender?
However, know that our code is an SDK => Do we have any node code on github now?

---------

Confluent

The Schema Registry enables safe, zero downtime evolution of schemas by centralizing the management of schemas written for the Avro
serialization system. It tracks all versions of schemas used for every topic in Kafka and only allows evolution of schemas according to
user-defined compatibility settings

The Schema Registry also includes plugins for Kafka clients that handle schema storage and retrieval for Kafka messages that are sent in the
Avro format.

An existing cluster can be upgraded easily by performing a rolling restart of Kafka brokers.

Start by adding the Schema Registry and updating your applications to use the Avro serializer. Next, add the REST Proxy to support
applications that may not have access to good Kafka clients or Avro libraries. Finally, run periodic Camus jobs to automatically load data
from Kafka into HDFS.


 formats like JSON lack a strictly defined format, which has two drawbacks. First, they are verbose because field names and type information
have to be explicitly represented in the serialized format. Second, the lack of structure makes consuming data in these formats more
challenging because fields can be arbitrarily added or removed.

t not only requires a schema during data serialization, but also during data deserialization. Because the schema is provided at decoding
time, metadata such as the field names donâ€™t have to be explicitly encoded in the data

data encoded with the old schema can be read with the newer schema. 
data encoded with the new schema can be read with the old schema. When projecting data written with the new schema to the old one, the new
field is simply dropped.  
