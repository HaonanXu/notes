Problem:

Design a system with Kafka so that in a multi-tenant environment, user can
1.define a topic and schema of the message to upload. Note that user can change the schema of a topic's message
2.upload the messages, the Kafka consumer will process the messages and write the result to the persistant layer. Note that the processing pipleline may involve more read/write to different topics
3.We should be aware of the order of actions: i.e., between different upload, between schema defining and and uploads, and between schema definitions
4. Somehow track the progess of current load and be notified of errors

Uploading and defining schema should be done asynchronously,i.e., requests will go through the Kafka cluster before evaled and applied

Discussion

Because we need to do validation on a row bais and the op most likely is NOT stateless, it may be hard to work directly work with import file handle

1. how do we paritition the import data?
To meet our performance goal: import data can not be processed by a single consumer ,e.g., easily meet the target with 5 parititons, hopeless with 1

2. Interactions: can not edit the schema/import if another one is under way
this suggest that editing/importing same data should put into a same partition, so that we can enforce the ordering. i.e., partition scheme needs enforce that
=> i.e. a table ops topics so that we keep the order of ALL things happened to that table, so when we get an import request, we need to add the import start, and by symmery, import end when it ends

3.After we decide to kick off import, and starts loading message, what happens in between?

Since it has the ordering, maybe we can force the import data to coming after the ACK put into the actual data loading queue, Or, just make sure the payload doesnt get into until we are sure it is ok to kick off data, probably possible, because we can process the data really quickly, can pub this to a consumer saying reading to ACC payloads

Now, multi-tenant case, how will this affect our partition scheme.

Recall Kafka's limitation on over all # of partitions => our partitions shouldnt be fine grained in the end, i.e., if we have topic/partition specific, the # of partitions will scale linearing with our # of clients, this COULD be a potential problem, i.e., 100 tenents => 100 * partitions specific to the tenant
Plus, a single consumer may read from multiple partitions on high level consumer (when you have more partitions than threads, some threads may get data from multiple partitions) so the multi-tenantness must be built in. But building multi-tenantness in for all consumer is hard! => but # of paritions will quickly grow => we will have to add a new cluster


Insights so far
1. each import must be processed in parallel
2. actions done to a single table must go into the same partition, so we can process in the order they are announced. 
3. putting all tenants into a single cluster is most likely impractical because A. limit on # of partitions, B.inherently multi-tenantness on all consumer constraint is too hard

------
How to track the progress?
This idea is more akin to batch processing
producer: can get offset of the first message sent in the message set
consumer: each message has an offset associated with it 
so as local state, very easy to track how many messages are processed based on offset. Need a way to recycle and clean up local states probably use timeouts
reading status will have to be done through in memory store => whoever requests the process will just aggregate the result
or simplistic approach: just when producer is done, mark progress steps

The nature of progress would be mean that it would be a good fit for in-memory store => initiator populates and reads from it, when it knows it is done => delete it

--------
How to track/handle errors?
When error happens
1.how do we notify consumer to stop, and to an extend, the initiator of the stream the error?
so producer needs to be aware of some state

2.how to deal with unprocessed?
If we ignore all remaining upload, our mentality is still batch processing
i.e., probably will record the current offending one and move on
probably can NOT use in memory store to store errors due to # of messages => i.e. needs go to Kafka as well, and whoever inits the action will listen to the specific partition

We may put an error in another log for further analysis, although the final format may just be a file with handle passed to user

----------
How do we organize topics?
probalby one topic for import data, too specific topic info will cause problem of too many partitions
one package needs to "slice" the data for parallel processing
then pump the data into the pipe with both start and end message

This means each partition may have multiple table's import data => may not be a wise idea,i.e., why not limit it so that between start and end we expect data from only 1 table,i.e., local state involves only 1 table. Also notice that, a start message might be redundant => EoF class message is good enough.
This is essentially transforming the streams into mini-batches

But a lot of assumptions are bad if our sources are streams!i.e., there is no "EoF", and we PROBABLY can not do blocking processing only 1 target due to production rates, i.e., consumer needs to handle input to all sources at the same time (!!)



