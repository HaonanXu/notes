active/eager/push/pessimistic replication
1. client sends the reuqest
2. sync portion of replication: the first server contacts the two other servers and watis until it has received replied from all the other
servers
3. sends response to client result
=>problem: slowest is the bottleneck, can not tolerate loss of any servers, but strong guarantees

Primary/backup replication: 1PC
all updates are on the primary and a lot of ops is shipped across the network to replicas
Any async replication algo can only provide weak durability guarantees: if the ppirmary fails, thent he updates that have yet been sent to
the backups are lost
However, the sync version can cause incorrect data: lost res,(incorrect update problem)

Improvement: 2PC
Assumption: data in stable storage is never lost. No node cashes forever
1. coordinate sends update to all the particiepants
2. if voteing to commit, the pariticpatns store the update onto WAL/temp area
3. coorddinator decides the outcome and informs everyone about it
4.If all participants voted to commit, then the update is taken from the temp aran and made permanent
5. can not survive simultaneous failure of the coordinator and a node during a commit

IMPORTANT: recovery procedures during node failures

For single-copy consistency: network parition tolerance requires during such partiions, only 1 partition of the system remains active, since
it is impossible to prevent divergence

Simple fixed leader or master server is optimiation, upon its failure, elect a new leader
epochs act as a logical clock so that other nodes can identify when anoutdated node starts communicating

1.leader mains heartbeat which allows the followers to detect if the leader fails or beocmes partitioned
2.when a node detects a leader has become no-responsive, go into candiate state, and increment the epoch value by 1
3. a node recieve a majority of the votes becomes the new leader: vote as in first-come first-served. candidates themselves waits a random
time before bids


Practical considerations
1. repeated leader election
2. repeated propose messages
3. followers and proposers do not lose items in stable storage/not corrupted
4. cluster membership change. Majority rule does not hold if the membership change change arbitrarility
5. bring replica up to date in a safe and efficient manner
6. snapshoting and GCing data. required to guarantee satefy after some reasonable period


Reading list
-------
Concurrency Control and Recovery in Database Systems
How to build a highly available system with consensus
Reconfiguring a State Machine
In Search of an Understandable Consensus Algorithm
