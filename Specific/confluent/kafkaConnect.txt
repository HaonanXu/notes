the Connector => generating the set of Tasks and indicating to the framework when they need to be updated.

A Kafka Connect cluster consists of a set of Worker processes that are containers that execute Connectors and Tasks

, Kafka Connect performs broad copying by default by having users define jobs at the level of Connectors which then break the job into
smaller Tasks. 

Kafka Connect does require persistent storage for offset data to ensure it can recover from faults, and users will need to configure this
storage.

In distributed mode, you start many worker processes using the same group.id and they automatically coordinate to schedule execution of
connectors and tasks across all available workers. 

interaction with a distributed-mode cluster is via the REST API. To create a connector, you start the workers and then make a REST requestjo
to create a connector 

Kafka Connect workers do not have a special “leader” process that you have to interact with to use the REST API; all nodes can respond to
REST requests, including creating, listing, modifying, and destroying connectors.

#######
Need to set the following for distributed workers: 
1.group.id
2.config.storage.topic: Kafka topic to store connector and task config state. This topic should always have a single partition and be highly
replicated (3x or more).
3.offset.storage.topic: the Kafka topic to store connector offset state in. Should have large # of partitons (25 or 50)

In distributed mode, they will be included in the JSON payload for the request that creates (or modifies) the connector

How to install new plugins

Kafka Connect workers are configured by passing a properties file containing any required or overridden options as the first parameter to
the worker process.
