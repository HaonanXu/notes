A deleted column can reappear if you do not run node repair routinely.

 if a node is down longer than the grace period, the node can miss the delete because the tombstone disappears after gc_grace_seconds. Cassandra always attempts to replay missed updates when the node comes back up again. After a failure, it is a best practice to run node repair to repair inconsistencies across all of the replicas when bringing a node back into the cluster. If the node doesn't come back within gc_grace,_seconds, remove the node, wipe it, and bootstrap it again.

hinted handoff writes

If the Bloom filter does not rule out the SSTable, Cassandra checks the partition key cache and takes one of these courses of action:

If an index entry is found in the cache:
Cassandra goes to the compression offset map to find the compressed block having the data.
Fetches the compressed data on disk and returns the result set.

If an index entry is not found in the cache:
Cassandra searches the partition summary to determine the approximate location on disk of the index entry.
Next, to fetch the index entry, Cassandra hits the disk for the first time, performing a single seek and a sequential read of columns (a range read) in the SSTable if the columns are contiguous.
Cassandra goes to the compression offset map to find the compressed block having the data.
Fetches the compressed data on disk and returns the result set.

The Bloom filter grows to approximately 1-2 GB per billion partitions

By default, the partition summary is a sample of the partition index. The compression offset map grows to 1-3 GB per terabyte compressed. The more you compress data, the greater number of compressed blocks you have and the larger the compression offset table.

 Using the SizeTieredCompactionStrategy or DateTieredCompactionStrategy tends to cause data fragmentation when rows are frequently updated. The LeveledCompactionStrategy (LCS) was designed to prevent fragmentation under this condition. 

The row cache is not write-through. If a write comes in for the row, the cache for it is invalidated and is not be cached again until it is read again

 reserve lightweight transactions for those situations where they are absolutely necessary

It is possible in Cassandra to have a write operation report a failure to the client, but still actually persist the write to a replica.

 if multiple client sessions update the same columns in a row concurrently, the most recent update is the one that readers see.

there are no locking or transactional dependencies when concurrently updating multiple rows or tables

A user can pick and choose on a per operation basis how many nodes must receive a DML command or respond to a SELECT query

 All writes to a replica node are recorded both in memory and in a commit log on disk before they are acknowledged as a success. 

 Cassandra writes to all replicas of the partition key, even replicas in other data centers. The consistency level determines only the number of replicas that need to acknowledge the write success to the client application. Typically, a client specifies a consistency level that is less than the replication factor specified by the keyspace. 

The read consistency level specifies how many replicas must respond to a read request before returning data to the client application
