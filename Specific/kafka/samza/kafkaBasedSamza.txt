Log holds the ultimate truth. 

We need to refer to logs for final answer. This means streaming job should accept input only from log, and not read from another other sources,e.g., file system or db. 

All state changes samza job needs should be read from/write to samza job's local state 

Conversely, log should have all state changes required by the log consumer. This often means log holds ALL state changes 

----
Samza local state and fault tolerance

In-memory varaibles should be viewed as optimization similar to caching,i.e., removing them shouldnt affect your program's correctness 

At-least delivery means you need to performance action first, and then persist the state change in the local storage.

------
local state's k-v design

k-v design should not be based on the principle of (de)normalization. Instead, the expected query pattern should dictate design,i.e., instead of model  -> query, we have to do query -> model.

Denormalization: expensive update operation, because you potentially read a collection and do only minor changes, e.g., modify some fields

Normalization: in-memory joins

---
Log often is not presentable/consumerable. 

Therefore, we need to put them into another data sinks.
Common choices:
HDFS for immutable data
HBase/Cassandra to receive writes from streaming jobs, and reads from API implementation
ElasticSearch for search functions

Note that role of data sink is for scalable read. The sink itself should not perform much, if any, analytical work. That is the responsiblilty of data sink consumer

