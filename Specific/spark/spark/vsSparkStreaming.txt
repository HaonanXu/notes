Message model:
samza: call process() on every single message
spark: batches based on time intervals

Execution model:
spark: 
1. driver, which sits on application manager (cluster), talks to resource manager to allocate exectors
2. exectuors run taks sent by driver
3. Each receiver is a long-running task.
4. processing has a bunch of tasks

Ordering:
Spark:
ordered processing of batches => the exact ordering of messages is not important in Spark Streaming (!)
no at least once or at most once messaging sematics (!)
transformation operations to be deterministic
Samza:
process the messages as the order they appear in the partition of the stream
at-least-once message delivery

State Management
Spark:
get a new DStream via updateStateByKey => NO key-value access to the data(!)
every time a new batch is processed, Spark Streaming conusmes the entire state DStream to update key and value(!)
state RDD is written into the HDFS after every checkpoint interval => very expensive when the state becomes large
join only two batches that are in the same time interval, doesnt handel where events in two streams mismatch
no processer can not be too slower than receiver, otherwise it will build up

Fault tolerance:
spark:
when a worker node fails, restart by the cluster manager
when a driver node fails in spark streaming, standalone will restart the driver node. Not supported in yarn and mesos model for now
In the falure happens when thedata is received but not yet replicated to other node => already fixed by writeahead log by SPARK-3129 in 1.2
Samza: 
stored lastest processed message offset, and always commit the checkpoint after processing the data

Workflow:
spark:
build entire processing graph with DSL and deploy then entire graph, communication between the nodews in the form of DStreams
a job's output can NOT be consume by mutliple unrelated jobs => not intended to be used in away where one topology's 
=> spark writing to kafka??

