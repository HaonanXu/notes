simple rewind: restaring the job at an older point in time in the stream, and running through all messages since that time

parallel rewind:
run two jobs in parallel: one job continues to handle live updates with low latency while the other is started at an older point in the stream => eventually the reprocessing job catches up with the real-time job

to jump back, you can
1. stop the job and change the last check points
2. start a new job with a different job.name or job.id => but it will start at the oldest offset available

In most cases, a job that jumps back in time should start with an empty state. can reset state by deleting the topic, or by changing the name of the topic in job config

common to kafka brokers to keep one or two weeks of message history acceissible, even for high volume topics => can handle TBs of history

activity events: time-based retention, events older than the retention period are deleted or archived in HDFS

database changes: last change matters => changelog streams used by samza's persistent state => but need to jump to the beginning of time with log compaction

To implement parallel rewind
1. need different job.name or job.id
2. need different changelog to maintain persistant state
3. each job to have separarte output stream, or if you use same output, newer data can NOT be overriden by older data
4. new output test case ideal for A/B testing
5. eventually need to shut down the old job, delete the checkpoint and changelog streams belonging to the old version

------
What is data/known?
1.
changelog: in this context, just kafka topics

2.
persistant state for samza:
stored on disk, on the same machine as the stream task. 
replicated for fault tolerance. No problem check pointing large amounts of state
all writes to the database partition are replicated to a durable kafka. So when a mahcine fails, we can restart the tasks on another machine, and consume the changelog to restore the contents of the database partition (50 MB/sec)

3.
log compaction: kafka concept
---------
Discussion:
2.
With updated rules do we start with a new job id?
Yes, then our offset will start from beginning
No, then we need to somehow rewind offset so we can reprocess (some of) the past data

3. Our new job will write to a new output stream?
if so, then we need to start new downstream jobs, to notify of input stream change, because once a job is running, it is config is immutable
this also means we will probably delete the old output stream in the end

if to same topic, then the downstream job must be able to handle the newly updated data. This requirement seems common requirement of stream jobs

4.when we are inside reprocessing logic, can we avoid recalcing everything?
	when we read each message => ignore uninteresting ones
	or dont read all messages => only partitions that we know will change. However, you can not specifiy which partitions to read


Devise a simpler auxilary problem:
just rewind to start, what do we need to do?
1.How do we know where the job tarball/config file is located
2.need to set config to ignore saved check points, => a new job id is good enough

So we need new config file for sure, even for different job id => probalby have to generate one
