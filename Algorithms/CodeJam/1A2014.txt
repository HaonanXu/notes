notice that flipping bits are independent. Also, if we have a solution, no
more than half of the bits are flipped, i.e., at most 20 flipped in the
solution
for each bit column, if #0s and #1s are different, we know for sure to flip or not
if they are same....

or think horizontally
find the mapping for source i to target j => 150^2 choices
for each choice, we can infer a flipping order, verify this flipping
order,i.e.,

for each mapping
	get fillping order
	for each row
		apply map to row
		if mapped is occupied, error
		else marked as mapped

total cost will be 150*150*40

--------
Input: N <=1000

in a FBT, 1 node with deg 2, s with deg 3, s+1 with degree 1

for each level
	leaf node: delete obvious nos, so its parent has degree at most 3

	for all degree 2 nodes
	if parent is 2 as well, delete current one's and its child,update degree
	leave deg three untouched
	add current parent to next level

Hard to code, plus something seems wrong.

Notice its BST, probably some form of recursion? 
we know the root is special=> only 1, but which one? Try them all!

Cost(v1, v2) = cost to make v1 FBT with its parent coming from v2

for each neighbor m of v1 but not v2
	calculate cost(m, v1)

cost(v1, v2) = 2 min cost(m,v1) => answer is trivial if 0 neighbor after v2, answer is invalid if 1 neighbor

feasible because our matrix has at most 1mil entries

psudocode

main:
for each v in V
	if(deg(v) < 2)
		continue;

	pick v as root
	
	foreach u in neighbor(v)
		Compute cost(u, v)
		keep the 2 min costs

	if has 2 mins costs
		update global best result
end


cost(u,v):
	if(deg(u) == 1)
		return 0 //leaf node
	else if(deg(u) == 2)
		return count(u, v) //have to sacrifice child
	
	//dp cache here
	
	foreach w in neighbor(u) - {v}
		Compute cost(w, u)
		keep the 2 min costs

	if has 2 min costs
		upldate local best result

	return localbest result and populate dp cache
end

count(u, v):
	if deg(u) == 1
		return 1

	//dp cache here
	for each w in neighbor(u) - {v}
		total count+= count(w, u)

	
	return total count and populate dp cache
end

----
2 Official answer:

Insight: calculate min deletion is equivalent of calculating max retention => computation becomes much easier. So my count part is not needed

linear solution: pre-compute 3 largest subtree

each pre-computation part has
node, parent, largest 3 childs along with its size, pick a random root node and start DFSing

how do we fix the root node? we will try to move the root to a child position, and see if it can improve overall size count,e.g., if root become first level child, second level child....etc. This means we need to try to make each node root once, note if we do DFS/BFS, we can reuse previous step's calculation result=> instant calc. Another insight is that, when we are making a root, the state in the root would be enough the determine the total maxsize, even though we do not know the arrangement of the rotated out root

	

--------
(HARD)
One way is to generate many permutations, check its distribution of "scores"

Why is standard algo good? i has 1/n chance to be on each position j

proof:
1st element: trivial proof
2nd element: Pr(2 at 1) = 1/N, other case (1 - (1/N)) * 1/N-1 = 1/N
3rd element: Pr(3 at 1) = 1/N, Pr(3 at 2) = Pr(2 at 3) = 1/N //induciton here....

Why is "swap with all" bad? 
The difference is that at each step, we can swap with previous index, compared to the perfect approach, the means j is more likley to be swapped under j => how to build a score function

another approach: naive bayes classifier
P(GOOD|S)
